// This file was generated by BarrettCUDA v0.1.
// 
// BarrettCUDA is a fast(ish) implementation of finite field sparse
// matrix-vector multiplication (SpMV) for Nvidia GPU devices, written
// in CUDA C++. BarrettCUDA supports SpMV for matrices expressed in
// the 'compressed column storage' (CCS) sparse matrix representation
// over (i) the field of integers modulo an arbitrary multi-precision
// prime, or (ii) either of the binary fields GF(2^8) or GF(2^16).
// 
// Copyright (C) 2016, Ryan Henry and Syed Mahbub Hafiz.
// 
// BarrettCUDA is free software: you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published
// by the Free Software Foundation, either version 3 of the License,
// or (at your option) any later version.
// 
// BarrettCUDA is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.
// 
// You should have received a copy of the GNU General Public License
// along with BarrettCUDA. If not, see <http://www.gnu.org/licenses/>.

#ifndef __UINT_128_H
#define __UINT_128_H
#include "uint.h"

#ifndef __UINTX__
#define __UINTX__
    typedef uint128 uintX;
#endif

#define LIMBS_PER_UINTX 4

static inline NTL::ZZ to_ZZ(const uint128 & n)
{
    return to_ZZ<uint128>(n);
}

static inline NTL::ZZ_p to_ZZ_p(const uint128 & n)
{
    return NTL::to_ZZ_p(to_ZZ<uint128>(n));
}

static inline void to_uint128(const NTL::ZZ & n, uint128 & ret)
{
    to_uint<uint128>(n, ret);
}

__device__ __forceinline__ void normalize(uint128 & a_lo, uint128 & a_hi,
	const uint128 & s_lo, const uint s_hi)
{
    asm("sub.cc.u32	 %0, %0, %8;\n\t"	// r0-= r8
	"subc.cc.u32	 %1, %1, %9;\n\t"	// r1-=( r9+c)
	"subc.cc.u32	 %2, %2,%10;\n\t"	// r2-=(r10+c)
	"subc.cc.u32	 %3, %3,%11;\n\t"	// r3-=(r11+c)
	"subc.cc.u32	 %4, %4,%12;\n\t"	// r4-=(r12+c)
	"subc.cc.u32	 %5, %5,%12;\n\t"	// r5-=(r12+c)
	"subc.cc.u32	 %6, %6,%12;\n\t"	// r6-=(r12+c)
	"subc.u32	 %7, %7,%12;\n\t"	// r7-=(r12+c)
	: "+r"(a_lo.x), "+r"(a_lo.y), "+r"(a_lo.z), "+r"(a_lo.w),
	  "+r"(a_hi.x), "+r"(a_hi.y), "+r"(a_hi.z), "+r"(a_hi.w)
	: "r"(s_lo.x), "r"(s_lo.y), "r"(s_lo.z), "r"(s_lo.w), "r"(s_hi));
}

__device__ __forceinline__ uint sub(uint128 & a_lo, uint128 & a_hi,
	const uintXp<uint128> & r)
{
    asm("sub.cc.u32	 %0, %0, %5;\n\t"	// r0-= r5
	"subc.cc.u32	 %1, %1, %6;\n\t"	// r1-=( r6+c)
	"subc.cc.u32	 %2, %2, %7;\n\t"	// r2-=( r7+c)
	"subc.cc.u32	 %3, %3, %8;\n\t"	// r3-=( r8+c)
	"subc.u32	 %4, %4, %9;\n\t"	// r4-=( r9+c)
	: "+r"(a_lo.x), "+r"(a_lo.y), "+r"(a_lo.z), "+r"(a_lo.w),
	  "+r"(a_hi.x)
	: "r"(r.lo.x), "r"(r.lo.y), "r"(r.lo.z), "r"(r.lo.w), "r"(r.hi));
    return a_hi.x;
}

__device__ __forceinline__ uintXp<uint128> get_q(const uint128 & a_lo,
	const uint128 & a_hi, const uintXp<uint128> & mu)
{
    uint __attribute__((unused)) tmp0;
    uint __attribute__((unused)) tmp1;
    uintXp<uint128> q = {0};
    asm("mul.hi.u32	 %3, %7,%12    ;\n\t"	// r3 =[ r7*r12].hi   (r-3=>r3)
	"mad.lo.cc.u32	 %3, %8,%12, %3;\n\t"	// r3+=[ r8*r12].lo   (r-3=>r3)
	"madc.lo.u32	 %4, %9,%12,  0;\n\t"	// r4 =[ r9*r12].lo+c (r-2=>r4)
	"mad.lo.cc.u32	 %3, %7,%13, %3;\n\t"	// r3+=[ r7*r13].lo   (r-3=>r3)
	"madc.lo.cc.u32	 %4, %8,%13, %4;\n\t"	// r4+=[ r8*r13].lo+c (r-2=>r4)
	"madc.lo.u32	 %5,%10,%12,  0;\n\t"	// r5 =[r10*r12].lo+c (r-1=>r5)
	"mad.lo.cc.u32	 %4, %7,%14, %4;\n\t"	// r4+=[ r7*r14].lo   (r-2=>r4)
	"madc.lo.cc.u32	 %5, %9,%13, %5;\n\t"	// r5+=[ r9*r13].lo+c (r-1=>r5)
	"madc.lo.u32	 %6,%11,%12,  0;\n\t"	// r6 =[r11*r12].lo+c
	"mad.hi.cc.u32	 %4, %8,%12, %4;\n\t"	// r4+=[ r8*r12].hi   (r-2=>r4)
	"madc.lo.cc.u32	 %5, %8,%14, %5;\n\t"	// r5+=[ r8*r14].lo+c (r-1=>r5)
	"madc.lo.cc.u32	 %6,%10,%13, %6;\n\t"	// r6+=[r10*r13].lo+c
	"madc.lo.u32	 %0,%11,%13,  0;\n\t"	// r0 =[r11*r13].lo+c
	"mad.hi.cc.u32	 %4, %7,%13, %4;\n\t"	// r4+=[ r7*r13].hi   (r-2=>r4)
	"madc.lo.cc.u32	 %5, %7,%15, %5;\n\t"	// r5+=[ r7*r15].lo+c (r-1=>r5)
	"madc.lo.cc.u32	 %6, %9,%14, %6;\n\t"	// r6+=[ r9*r14].lo+c
	"madc.lo.cc.u32	 %0,%10,%14, %0;\n\t"	// r0+=[r10*r14].lo+c
	"madc.lo.u32	 %1,%11,%14,  0;\n\t"	// r1 =[r11*r14].lo+c
	"mad.hi.cc.u32	 %5, %9,%12, %5;\n\t"	// r5+=[ r9*r12].hi   (r-1=>r5)
	"madc.lo.cc.u32	 %6, %8,%15, %6;\n\t"	// r6+=[ r8*r15].lo+c
	"madc.lo.cc.u32	 %0, %9,%15, %0;\n\t"	// r0+=[ r9*r15].lo+c
	"madc.lo.cc.u32	 %1,%10,%15, %1;\n\t"	// r1+=[r10*r15].lo+c
	"madc.lo.u32	 %2,%11,%15,  0;\n\t"	// r2 =[r11*r15].lo+c
	"mad.hi.cc.u32	 %5, %8,%13, %5;\n\t"	// r5+=[ r8*r13].hi   (r-1=>r5)
	"madc.hi.cc.u32	 %6,%10,%12, %6;\n\t"	// r6+=[r10*r12].hi+c
	"madc.hi.cc.u32	 %0,%11,%12, %0;\n\t"	// r0+=[r11*r12].hi+c
	"madc.hi.cc.u32	 %1,%11,%13, %1;\n\t"	// r1+=[r11*r13].hi+c
	"madc.hi.cc.u32	 %2,%11,%14, %2;\n\t"	// r2+=[r11*r14].hi+c
	"madc.hi.u32	 %3,%11,%15,  0;\n\t"	// r3 =[r11*r15].hi+c
	"mad.hi.cc.u32	 %5, %7,%14, %5;\n\t"	// r5+=[ r7*r14].hi   (r-1=>r5)
	"madc.hi.cc.u32	 %6, %9,%13, %6;\n\t"	// r6+=[ r9*r13].hi+c
	"madc.hi.cc.u32	 %0,%10,%13, %0;\n\t"	// r0+=[r10*r13].hi+c
	"madc.hi.cc.u32	 %1,%10,%14, %1;\n\t"	// r1+=[r10*r14].hi+c
	"madc.hi.cc.u32	 %2,%10,%15, %2;\n\t"	// r2+=[r10*r15].hi+c
	"addc.cc.u32	 %3, %3,  0    ;\n\t"	// r3+= c
	"addc.u32	 %4,  0,  0    ;\n\t"	// r4 = c
	"mad.hi.cc.u32	 %6, %8,%14, %6;\n\t"	// r6+=[ r8*r14].hi  
	"madc.hi.cc.u32	 %0, %9,%14, %0;\n\t"	// r0+=[ r9*r14].hi+c
	"madc.hi.cc.u32	 %1, %9,%15, %1;\n\t"	// r1+=[ r9*r15].hi+c
	"addc.cc.u32	 %2, %2,  0    ;\n\t"	// r2+= c
	"addc.cc.u32	 %3, %3,  0    ;\n\t"	// r3+= c
	"addc.u32	 %4, %4,  0    ;\n\t"	// r4+= c
	"mad.hi.cc.u32	 %6, %7,%15, %6;\n\t"	// r6+=[ r7*r15].hi  
	"madc.hi.cc.u32	 %0, %8,%15, %0;\n\t"	// r0+=[ r8*r15].hi+c
	"addc.cc.u32	 %1, %1,  0    ;\n\t"	// r1+= c
	"addc.cc.u32	 %2, %2,  0    ;\n\t"	// r2+= c
	"addc.cc.u32	 %3, %3,  0    ;\n\t"	// r3+= c
	"addc.u32	 %4, %4,  0    ;\n\t"	// r4+= c
	"mad.lo.cc.u32	 %6, %7,%16, %6;\n\t"	// r6+= r7*r16
	"madc.lo.cc.u32	 %0, %8,%16, %0;\n\t"	// r0+= r8*r16+c
	"madc.lo.cc.u32	 %1, %9,%16, %1;\n\t"	// r1+= r9*r16+c
	"madc.lo.cc.u32	 %2,%10,%16, %2;\n\t"	// r2+=r10*r16+c
	"madc.lo.cc.u32	 %3,%11,%16,%3;\n\t"	// r3+=r11*r16+c
	"addc.u32	 %4, %4,  0    ;\n\t"	// r4+=c
	: "+r"(q.lo.x), "=r"(q.lo.y), "=r"(q.lo.z), "=r"(q.lo.w),
	  "=r"(q.hi), "=r"(tmp0), "=r"(tmp1)
	: "r"(a_lo.w ), "r"(a_hi.x), "r"(a_hi.y), "r"(a_hi.z), "r"(a_hi.w),
	  "r"(mu.lo.x), "r"(mu.lo.y), "r"(mu.lo.z), "r"(mu.lo.w), "r"(mu.hi));

    return q;
}

__device__ __forceinline__ uintXp<uint128> get_r2(const uintXp<uint128> & q,
	const uint128 & modulus)
{
    uintXp<uint128> r = {0};
    asm("mad.lo.u32	 %0, %5,%10,  0;\n\t"	// r0 =[ r5*r10].lo  
	"mad.lo.u32	 %1, %5,%11,  0;\n\t"	// r1 =[ r5*r11].lo  
	"mad.lo.cc.u32	 %1, %6,%10, %1;\n\t"	// r1+=[ r6*r10].lo  
	"madc.lo.u32	 %2, %5,%12,  0;\n\t"	// r2 =[ r5*r12].lo+c
	"mad.hi.cc.u32	 %1, %5,%10, %1;\n\t"	// r1+=[ r5*r10].hi  
	"madc.lo.cc.u32	 %2, %6,%11, %2;\n\t"	// r2+=[ r6*r11].lo+c
	"madc.lo.u32	 %3, %5,%13,  0;\n\t"	// r3 =[ r5*r13].lo+c
	"mad.hi.cc.u32	 %2, %5,%11, %2;\n\t"	// r2+=[ r5*r11].hi  
	"madc.lo.cc.u32	 %3, %6,%12, %3;\n\t"	// r3+=[ r6*r12].lo+c
	"madc.lo.u32	 %4, %6,%13,  0;\n\t"	// r4 =[ r6*r13].lo+c
	"mad.lo.cc.u32	 %2, %7,%10, %2;\n\t"	// r2+=[ r7*r10].lo  
	"madc.hi.cc.u32	 %3, %5,%12, %3;\n\t"	// r3+=[ r5*r12].hi+c
	"madc.hi.cc.u32	 %4, %5,%13, %4;\n\t"	// r4+=[ r5*r13].hi+c
	"mad.hi.cc.u32	 %2, %6,%10, %2;\n\t"	// r2+=[ r6*r10].hi  
	"madc.lo.cc.u32	 %3, %7,%11, %3;\n\t"	// r3+=[ r7*r11].lo+c
	"madc.lo.cc.u32	 %4, %7,%12, %4;\n\t"	// r4+=[ r7*r12].lo+c
	"mad.hi.cc.u32	 %3, %6,%11, %3;\n\t"	// r3+=[ r6*r11].hi  
	"madc.hi.cc.u32	 %4, %6,%12, %4;\n\t"	// r4+=[ r6*r12].hi+c
	"mad.lo.cc.u32	 %3, %8,%10, %3;\n\t"	// r3+=[ r8*r10].lo  
	"madc.lo.cc.u32	 %4, %8,%11, %4;\n\t"	// r4+=[ r8*r11].lo+c
	"mad.hi.cc.u32	 %3, %7,%10, %3;\n\t"	// r3+=[ r7*r10].hi  
	"madc.hi.cc.u32	 %4, %7,%11, %4;\n\t"	// r4+=[ r7*r11].hi+c
	"mad.lo.cc.u32	 %4, %9,%10, %4;\n\t"	// r4+=[ r9*r10].lo  
	"mad.hi.cc.u32	 %4, %8,%10, %4;\n\t"	// r4+=[ r8*r10].hi  
	: "+r"(r.lo.x), "=r"(r.lo.y), "=r"(r.lo.z), "=r"(r.lo.w), "=r"(r.hi)
	: "r"(q.lo.x), "r"(q.lo.y), "r"(q.lo.z), "r"(q.lo.w), "r"(q.hi),
	  "r"(modulus.x), "r"(modulus.y), "r"(modulus.z), "r"(modulus.w));

    return r;
}

__device__ __forceinline__ void sub_modulus(uintXp<uint128> & r, const uint128 & m)
{
    asm("sub.cc.u32	 %0, %0, %5;\n\t"	// r0-= r5
	"subc.cc.u32	 %1, %1, %6;\n\t"	// r1-=( r6+c)
	"subc.cc.u32	 %2, %2, %7;\n\t"	// r2-=( r7+c)
	"subc.cc.u32	 %3, %3, %8;\n\t"	// r3-=( r8+c)
	"subc.u32	 %4, %4,  0;\n\t"	// r4-=(    c)
	: "+r"(r.lo.x), "+r"(r.lo.y), "+r"(r.lo.z), "+r"(r.lo.w), "+r"(r.hi)
	: "r"(m.x), "r"(m.y), "r"(m.z), "r"(m.w));
}

__device__ __forceinline__ void mad(uint128 & a_lo, uint128 & a_hi,
	uint & overflow, const uint128 & b, const uint128 & c)
{
    asm("mad.lo.cc.u32	 %0, %9,%13, %0;\n\t"	// r0+=[ r9*r13].lo  
	"madc.hi.cc.u32	 %1, %9,%13, %1;\n\t"	// r1+=[ r9*r13].hi  
	"madc.lo.cc.u32	 %2,%11,%13, %2;\n\t"	// r2+=[r11*r13].lo+c
	"madc.hi.cc.u32	 %3,%11,%13, %3;\n\t"	// r3+=[r11*r13].hi+c
	"madc.lo.cc.u32	 %4,%12,%14, %4;\n\t"	// r4+=[r12*r14].lo+c
	"madc.hi.cc.u32	 %5,%12,%14, %5;\n\t"	// r5+=[r12*r14].hi+c
	"madc.lo.cc.u32	 %6,%12,%16, %6;\n\t"	// r6+=[r12*r16].lo+c
	"madc.hi.cc.u32	 %7,%12,%16, %7;\n\t"	// r7+=[r12*r16].hi+c
	"addc.u32	 %8, %8,  0    ;\n\t"	// r8+=c
	"mad.lo.cc.u32	 %1,%10,%13, %1;\n\t"	// r1+=[r10*r13].lo  
	"madc.hi.cc.u32	 %2,%10,%13, %2;\n\t"	// r2+=[r10*r13].hi  
	"madc.lo.cc.u32	 %3,%12,%13, %3;\n\t"	// r3+=[r12*r13].lo+c
	"madc.hi.cc.u32	 %4,%12,%13, %4;\n\t"	// r4+=[r12*r13].hi+c
	"madc.lo.cc.u32	 %5,%12,%15, %5;\n\t"	// r5+=[r12*r15].lo+c
	"madc.hi.cc.u32	 %6,%12,%15, %6;\n\t"	// r6+=[r12*r15].hi+c
	"addc.cc.u32	 %7, %7,  0    ;\n\t"	// r7+=c
	"addc.u32	 %8, %8,  0    ;\n\t"	// r8+=c
	"mad.lo.cc.u32	 %1, %9,%14, %1;\n\t"	// r1+=[ r9*r14].lo  
	"madc.hi.cc.u32	 %2, %9,%14, %2;\n\t"	// r2+=[ r9*r14].hi  
	"madc.lo.cc.u32	 %3,%11,%14, %3;\n\t"	// r3+=[r11*r14].lo+c
	"madc.hi.cc.u32	 %4,%11,%14, %4;\n\t"	// r4+=[r11*r14].hi+c
	"madc.lo.cc.u32	 %5,%11,%16, %5;\n\t"	// r5+=[r11*r16].lo+c
	"madc.hi.cc.u32	 %6,%11,%16, %6;\n\t"	// r6+=[r11*r16].hi+c
	"addc.cc.u32	 %7, %7,  0    ;\n\t"	// r7+=c
	"addc.u32	 %8, %8,  0    ;\n\t"	// r8+=c
	"mad.lo.cc.u32	 %2,%10,%14, %2;\n\t"	// r2+=[r10*r14].lo  
	"madc.hi.cc.u32	 %3,%10,%14, %3;\n\t"	// r3+=[r10*r14].hi  
	"madc.lo.cc.u32	 %4,%11,%15, %4;\n\t"	// r4+=[r11*r15].lo+c
	"madc.hi.cc.u32	 %5,%11,%15, %5;\n\t"	// r5+=[r11*r15].hi+c
	"addc.cc.u32	 %6, %6,  0    ;\n\t"	// r6+=c
	"addc.cc.u32	 %7, %7,  0    ;\n\t"	// r7+=c
	"addc.u32	 %8, %8,  0    ;\n\t"	// r8+=c
	"mad.lo.cc.u32	 %2, %9,%15, %2;\n\t"	// r2+=[ r9*r15].lo  
	"madc.hi.cc.u32	 %3, %9,%15, %3;\n\t"	// r3+=[ r9*r15].hi  
	"madc.lo.cc.u32	 %4,%10,%16, %4;\n\t"	// r4+=[r10*r16].lo+c
	"madc.hi.cc.u32	 %5,%10,%16, %5;\n\t"	// r5+=[r10*r16].hi+c
	"addc.cc.u32	 %6, %6,  0    ;\n\t"	// r6+=c
	"addc.cc.u32	 %7, %7,  0    ;\n\t"	// r7+=c
	"addc.u32	 %8, %8,  0    ;\n\t"	// r8+=c
	"mad.lo.cc.u32	 %3,%10,%15, %3;\n\t"	// r3+=[r10*r15].lo  
	"madc.hi.cc.u32	 %4,%10,%15, %4;\n\t"	// r4+=[r10*r15].hi  
	"addc.cc.u32	 %5, %5,  0    ;\n\t"	// r5+=c
	"addc.cc.u32	 %6, %6,  0    ;\n\t"	// r6+=c
	"addc.cc.u32	 %7, %7,  0    ;\n\t"	// r7+=c
	"addc.u32	 %8, %8,  0    ;\n\t"	// r8+=c
	"mad.lo.cc.u32	 %3, %9,%16, %3;\n\t"	// r3+=[ r9*r16].lo  
	"madc.hi.cc.u32	 %4, %9,%16, %4;\n\t"	// r4+=[ r9*r16].hi  
	"addc.cc.u32	 %5, %5,  0    ;\n\t"	// r5+=c
	"addc.cc.u32	 %6, %6,  0    ;\n\t"	// r6+=c
	"addc.cc.u32	 %7, %7,  0    ;\n\t"	// r7+=c
	"addc.u32	 %8, %8,  0    ;\n\t"	// r8+=c
	: "+r"(a_lo.x), "+r"(a_lo.y), "+r"(a_lo.z), "+r"(a_lo.w),
	  "+r"(a_hi.x), "+r"(a_hi.y), "+r"(a_hi.z), "+r"(a_hi.w), "+r"(overflow)
	: "r"(b.x), "r"(b.y), "r"(b.z), "r"(b.w), "r"(c.x), "r"(c.y),
	  "r"(c.z), "r"(c.w));
}

#endif
